#include <cublas_v2.h> // æ”¾åœ¨æœ€ä¸Šé¢
#include <cstdio>
#include <cuda_runtime.h>
#include "sgemm.h"

// å®ï¼šæ£€æŸ¥ CUDA é”™è¯¯ (éå¸¸é‡è¦ï¼ŒCUDA æŠ¥é”™ä¸åƒ C++ é‚£ä¹ˆç›´æ¥)
#define CHECK_CUDA(func) \
{ \
    cudaError_t status = (func); \
    if (status != cudaSuccess) { \
        printf("CUDA Error at line %d: %s\n", __LINE__, cudaGetErrorString(status)); \
        exit(EXIT_FAILURE); \
    } \
}

// å®šä¹‰ Block å¤§å°ï¼Œå¿…é¡»å’Œ Host ç«¯è®¾ç½®çš„ä¸€æ ·
#define BLOCK_SIZE 16

// -----------------------------------------------------------
// 1. Kernel å‡½æ•°: åœ¨ GPU ä¸Šæ‰§è¡Œçš„ä»£ç 
// __global__ è¡¨ç¤º: Host(CPU) è°ƒç”¨ï¼ŒDevice(GPU) æ‰§è¡Œ
// -----------------------------------------------------------
__global__ void sgemm_naive_kernel(int N, float* A, float* B, float* C) {
    // æ¯ä¸€ä¸ªçº¿ç¨‹è®¡ç®— C çš„ä¸€ä¸ªå…ƒç´  C[row][col]
    
    // è®¡ç®—å½“å‰çº¿ç¨‹çš„åæ ‡
    // blockIdx: å½“å‰å±äºå“ªä¸ªæ–¹å—
    // blockDim: ä¸€ä¸ªæ–¹å—æœ‰å¤šå¤§
    // threadIdx: åœ¨æ–¹å—é‡Œçš„ç¼–å·
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < N && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < N; ++k) {
            sum += A[row * N + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}

// -----------------------------------------------------------
// 2. Host å‡½æ•°: è´Ÿè´£åˆ†é…æ˜¾å­˜ã€æ‹·è´æ•°æ®ã€å¯åŠ¨ Kernel
// -----------------------------------------------------------
void sgemm_gpu_naive(int N, float* A, float* B, float* C) {
    size_t bytes = N * N * sizeof(float);

    // 1. åœ¨ GPU (Device) ä¸Šåˆ†é…å†…å­˜
    float *d_A, *d_B, *d_C;
    CHECK_CUDA(cudaMalloc(&d_A, bytes));
    CHECK_CUDA(cudaMalloc(&d_B, bytes));
    CHECK_CUDA(cudaMalloc(&d_C, bytes));

    // 2. å°†æ•°æ®ä» CPU (Host) æ‹·è´åˆ° GPU (Device)
    // æ³¨æ„: cudaMemcpy æ˜¯åŒæ­¥æ“ä½œï¼Œæ¯”è¾ƒæ…¢ï¼Œä¼šè¢«è®¡å…¥æ—¶é—´
    CHECK_CUDA(cudaMemcpy(d_A, A, bytes, cudaMemcpyHostToDevice));
    CHECK_CUDA(cudaMemcpy(d_B, B, bytes, cudaMemcpyHostToDevice));
    
    // 3. é…ç½®å¯åŠ¨å‚æ•°
    // æ¯ä¸ª Block å¤§å°ä¸º 32x32 (1024ä¸ªçº¿ç¨‹ï¼Œè¿™æ˜¯ä¸Šé™)
    dim3 threadsPerBlock(32, 32);
    // Grid å¤§å°æ ¹æ® N åŠ¨æ€è®¡ç®—
    dim3 numBlocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x, 
                   (N + threadsPerBlock.y - 1) / threadsPerBlock.y);

    // 4. å¯åŠ¨ Kernel
    sgemm_naive_kernel<<<numBlocks, threadsPerBlock>>>(N, d_A, d_B, d_C);

    // æ£€æŸ¥ Kernel æ˜¯å¦å¯åŠ¨å¤±è´¥
    CHECK_CUDA(cudaGetLastError());
    
    // ç­‰å¾… GPU æ‰§è¡Œå®Œæ¯• (å› ä¸º Kernel æ˜¯å¼‚æ­¥çš„)
    CHECK_CUDA(cudaDeviceSynchronize());

    // 5. å°†ç»“æœä» GPU æ‹·å› CPU
    CHECK_CUDA(cudaMemcpy(C, d_C, bytes, cudaMemcpyDeviceToHost));

    // 6. é‡Šæ”¾æ˜¾å­˜
    CHECK_CUDA(cudaFree(d_A));
    CHECK_CUDA(cudaFree(d_B));
    CHECK_CUDA(cudaFree(d_C));
}


// -----------------------------------------------------------
// 2. Shared Memory Tiling Kernel
// -----------------------------------------------------------
__global__ void sgemm_shared_mem_kernel(int N, float* A, float* B, float* C) {
    // blockIdx: å½“å‰ Block çš„åæ ‡
    // threadIdx: å½“å‰çº¿ç¨‹åœ¨ Block å†…çš„åæ ‡
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    // è®¡ç®—å½“å‰çº¿ç¨‹è´Ÿè´£è®¡ç®— C ä¸­çš„å“ªä¸ªå…ƒç´ åæ ‡
    int row = by * BLOCK_SIZE + ty;
    int col = bx * BLOCK_SIZE + tx;

    // å£°æ˜ Shared Memory (è¿™æ˜¯ Block å†…æ‰€æœ‰çº¿ç¨‹å…±äº«çš„)
    // å¤§å°æ˜¯ 32x32 çš„ float çŸ©é˜µ
    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];

    float Cvalue = 0.0f;

    // æ ¸å¿ƒå¾ªç¯ï¼šå°†å¤§çŸ©é˜µæ‹†åˆ†æˆä¸€ä¸ªä¸ª BLOCK_SIZE å®½åº¦çš„â€œæ¡â€ (Tile) æ¥éå†
    // ph (Phase) ä»£è¡¨å½“å‰å¤„ç†ç¬¬å‡ ä¸ª Tile
    for (int ph = 0; ph < N / BLOCK_SIZE; ++ph) {
        
        // --- 1. åä½œåŠ è½½æ•°æ®åˆ° Shared Memory ---
        
        // æ¯ä¸ªçº¿ç¨‹è´Ÿè´£æ¬è¿ A çŸ©é˜µçš„ä¸€ä¸ªç‚¹ï¼šA[row][ph * BLOCK_SIZE + tx]
        As[ty][tx] = A[row * N + ph * BLOCK_SIZE + tx];
        
        // æ¯ä¸ªçº¿ç¨‹è´Ÿè´£æ¬è¿ B çŸ©é˜µçš„ä¸€ä¸ªç‚¹ï¼šB[ph * BLOCK_SIZE + ty][col]
        Bs[ty][tx] = B[(ph * BLOCK_SIZE + ty) * N + col];

        // ğŸš§ çº¿ç¨‹åŒæ­¥æ …æ  (å¿…è€ƒç‚¹!) ğŸš§
        // å¿…é¡»ç­‰å¾… Block å†…æ‰€æœ‰çº¿ç¨‹éƒ½æŠŠæ•°æ®æ¬å®Œäº†ï¼Œæ‰èƒ½å¼€å§‹è®¡ç®—
        __syncthreads();

        // --- 2. åœ¨ Shared Memory ä¸Šè¿›è¡Œè®¡ç®— ---
        
        for (int k = 0; k < BLOCK_SIZE; ++k) {
            // ç°åœ¨æ˜¯ä»é«˜é€Ÿçš„ Shared Memory (As, Bs) å–æ•°ï¼Œè€Œä¸æ˜¯æ…¢é€Ÿçš„ A, B
            Cvalue += As[ty][k] * Bs[k][tx];
        }

        // ğŸš§ å†æ¬¡åŒæ­¥ ğŸš§
        // å¿…é¡»ç­‰å¾…æ‰€æœ‰çº¿ç¨‹éƒ½ç®—å®Œäº†å½“å‰è¿™ä¸ª Tileï¼Œæ‰èƒ½è¿›å…¥ä¸‹ä¸€è½®å¾ªç¯å»è¦†ç›– As, Bs
        __syncthreads();
    }

    // å†™å›ç»“æœ
    if (row < N && col < N) {
        C[row * N + col] = Cvalue;
    }
}

// å¯¹åº”çš„ Host è°ƒç”¨å‡½æ•°
void sgemm_gpu_shared(int N, float* A, float* B, float* C) {
    size_t bytes = N * N * sizeof(float);
    float *d_A, *d_B, *d_C;

    CHECK_CUDA(cudaMalloc(&d_A, bytes));
    CHECK_CUDA(cudaMalloc(&d_B, bytes));
    CHECK_CUDA(cudaMalloc(&d_C, bytes));

    CHECK_CUDA(cudaMemcpy(d_A, A, bytes, cudaMemcpyHostToDevice));
    CHECK_CUDA(cudaMemcpy(d_B, B, bytes, cudaMemcpyHostToDevice));

    dim3 threadsPerBlock(BLOCK_SIZE, BLOCK_SIZE);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);

    // è°ƒç”¨æ–°çš„ Kernel
    sgemm_shared_mem_kernel<<<numBlocks, threadsPerBlock>>>(N, d_A, d_B, d_C);

    CHECK_CUDA(cudaGetLastError());
    CHECK_CUDA(cudaDeviceSynchronize());
    CHECK_CUDA(cudaMemcpy(C, d_C, bytes, cudaMemcpyDeviceToHost));

    CHECK_CUDA(cudaFree(d_A));
    CHECK_CUDA(cudaFree(d_B));
    CHECK_CUDA(cudaFree(d_C));
}


// 3. CuBLAS ç‰ˆæœ¬ (å®˜æ–¹é—­æºåº“)
void sgemm_gpu_cublas(int N, float* A, float* B, float* C) {
    cublasHandle_t handle;
    cublasCreate(&handle);

    float alpha = 1.0f;
    float beta = 1.0f; // æ³¨æ„è¿™é‡Œ beta=1ï¼Œæ„å‘³ç€ C += A*Bï¼Œç¬¦åˆæˆ‘ä»¬çš„ benchmark é€»è¾‘
    // å¦‚æœæƒ³è¦çº¯å‡€çš„ C = A*Bï¼Œbeta åº”è¯¥è®¾ä¸º 0ï¼Œä¸”å¤–éƒ¨ benchmark éœ€è¦æ¸…é›¶ C

    // å…³é”®ç‚¹ï¼šCuBLAS é»˜è®¤æ˜¯åˆ—ä¸»åº (Column Major)ï¼Œè€Œæˆ‘ä»¬æ˜¯è¡Œä¸»åº (Row Major)ã€‚
    // C = A * B (Row Major) ç­‰ä»·äº C^T = B^T * A^T (Column Major)
    // æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬éœ€è¦â€œéª—â€ä¸€ä¸‹ CuBLASï¼š
    // ä¼ è¿›å» B å½“ä½œ Aï¼Œä¼ è¿›å» A å½“ä½œ Bï¼Œæœ€åç®—å‡ºæ¥çš„ç»“æœç›´æ¥å°±æ˜¯è¡Œä¸»åºçš„ Cã€‚
    
    // è§£é‡Šå‚æ•°ï¼š
    // Handle, OP_N (ä¸è½¬ç½®), OP_N (ä¸è½¬ç½®), 
    // M=N, N=N, K=N (çŸ©é˜µå¤§å°),
    // alpha, 
    // B (ä½œä¸ºç¬¬ä¸€ä¸ªçŸ©é˜µ), ldb=N, 
    // A (ä½œä¸ºç¬¬äºŒä¸ªçŸ©é˜µ), lda=N, 
    // beta, 
    // C (ç»“æœ), ldc=N
    
    // æ˜¾å­˜æŒ‡é’ˆ
    size_t bytes = N * N * sizeof(float);
    float *d_A, *d_B, *d_C;
    cudaMalloc(&d_A, bytes);
    cudaMalloc(&d_B, bytes);
    cudaMalloc(&d_C, bytes);
    
    cudaMemcpy(d_A, A, bytes, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, B, bytes, cudaMemcpyHostToDevice);
    // C è¿™é‡Œå¦‚æœæ˜¯ç´¯åŠ ï¼Œéœ€è¦æŠŠ Host çš„ C æ‹·è¿›å»ï¼›å¦‚æœæ˜¯è¦†ç›–ï¼Œåˆ™ä¸éœ€è¦ã€‚
    // ä¸ºäº†å…¬å¹³ï¼Œå‡è®¾æ˜¯è¦†ç›– (beta=0) æˆ–ç´¯åŠ ã€‚è¿™é‡Œç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾æ˜¯çº¯è®¡ç®—ã€‚
    // ä¿®æ­£ï¼šbenchmark loop é‡Œæˆ‘ä»¬é€šå¸¸æŠŠ C è®¾ä¸º 0ï¼Œæ‰€ä»¥è¿™é‡Œ beta=0 æ¯”è¾ƒåˆé€‚ã€‚
    float beta_overwrite = 0.0f;

    cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, 
                N, N, N, 
                &alpha, 
                d_B, N, // B æ¢åˆ°å‰é¢
                d_A, N, // A æ¢åˆ°åé¢
                &beta_overwrite, 
                d_C, N);

    cudaDeviceSynchronize(); // ç­‰å¾…è®¡ç®—å®Œæˆ

    cudaMemcpy(C, d_C, bytes, cudaMemcpyDeviceToHost);

    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);
    cublasDestroy(handle);
}


// =========================================================
// æ–°å¢ï¼šçº¯è®¡ç®—æ¥å£ (Device Pointers Only)
// è¿™äº›å‡½æ•°å‡è®¾ d_A, d_B, d_C å·²ç»åœ¨æ˜¾å­˜é‡Œäº†ï¼Œåªè´Ÿè´£è®¡ç®—
// =========================================================

// 1. Naive Device æ¥å£
void sgemm_gpu_naive_device(int N, float* d_A, float* d_B, float* d_C) {
    dim3 threadsPerBlock(32, 32);
    dim3 numBlocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x, 
                   (N + threadsPerBlock.y - 1) / threadsPerBlock.y);
    sgemm_naive_kernel<<<numBlocks, threadsPerBlock>>>(N, d_A, d_B, d_C);
}

// 2. Shared Memory Device æ¥å£ (Block Size = 16)
void sgemm_gpu_shared_device(int N, float* d_A, float* d_B, float* d_C) {
    // å¼ºåˆ¶ä½¿ç”¨ Block Size 16 (é…åˆä¹‹å‰çš„å®å®šä¹‰)
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);
    sgemm_shared_mem_kernel<<<numBlocks, threadsPerBlock>>>(N, d_A, d_B, d_C);
}

// 3. CuBLAS Device æ¥å£
void sgemm_gpu_cublas_device(int N, float* d_A, float* d_B, float* d_C) {
    // ä¸ºäº†æ€§èƒ½ï¼Œhandle åº”è¯¥åœ¨å¤–éƒ¨åˆ›å»ºï¼Œä½†è¿™é‡Œä¸ºäº†æ¥å£ç®€å•å…ˆæ”¾åœ¨è¿™
    // æ³¨æ„ï¼šé¢‘ç¹åˆ›å»º handle ä¹Ÿæœ‰å¼€é”€ï¼Œä½†åœ¨å¤§çŸ©é˜µä¸‹å¯ä»¥å¿½ç•¥
    cublasHandle_t handle;
    cublasCreate(&handle);
    float alpha = 1.0f;
    float beta = 0.0f; 

    // å†æ¬¡æé†’ï¼šCuBLAS æ˜¯åˆ—ä¸»åºï¼Œæˆ‘ä»¬äº¤æ¢ A/B æ¥æ¬ºéª—å®ƒ
    cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, 
                N, N, N, 
                &alpha, 
                d_B, N, 
                d_A, N, 
                &beta, 
                d_C, N);
    
    // è¿™é‡Œä¸éœ€è¦ DeviceSynchronizeï¼Œå› ä¸º benchmark ä¸»ç¨‹åºä¼šåš
    cublasDestroy(handle);
}


// =========================================================
// Kernel 3: Vectorized Memory Access (float4)
// =========================================================

// å¼ºåˆ¶å°† float* è½¬æ¢ä¸º float4* è¯»å–
__global__ void sgemm_vectorized_kernel(int N, float* A, float* B, float* C) {
    // è¿™é‡Œçš„ x ä»£è¡¨â€œå‘é‡â€çš„åæ ‡ï¼Œæ¯ä¸ª x å¤„ç† 4 ä¸ª float
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x; // col æ˜¯å‘é‡ç´¢å¼•

    // å®é™…çš„åˆ—åæ ‡éœ€è¦ x4
    int actual_col = col * 4;

    if (row < N && actual_col < N) {
        // Cvalue ç”¨ float4 æ¥å­˜ï¼Œä¸€æ¬¡ç®— 4 ä¸ªç»“æœ
        float4 c_res = make_float4(0.0f, 0.0f, 0.0f, 0.0f);

        for (int k = 0; k < N; ++k) {
            float a_val = A[row * N + k]; // A è¿˜æ˜¯ä¸€ä¸ªä¸€ä¸ªè¯»

            // B ä¸€æ¬¡è¯» 4 ä¸ªï¼ (å…³é”®ä¼˜åŒ–)
            // æˆ‘ä»¬è¦æ±‚ N æ˜¯ 4 çš„å€æ•°ï¼Œä¸” B çš„åœ°å€å¯¹é½
            float4 b_val = reinterpret_cast<float4*>(&B[k * N + actual_col])[0];

            // æ‰‹åŠ¨å±•å¼€è®¡ç®— 4 ä¸ªç‚¹
            c_res.x += a_val * b_val.x;
            c_res.y += a_val * b_val.y;
            c_res.z += a_val * b_val.z;
            c_res.w += a_val * b_val.w;
        }

        // ç»“æœä¸€æ¬¡æ€§å†™å› 4 ä¸ª
        reinterpret_cast<float4*>(&C[row * N + actual_col])[0] = c_res;
    }
}

// Host å‡½æ•°
void sgemm_gpu_vectorized_device(int N, float* d_A, float* d_B, float* d_C) {
    // Block è¿˜æ˜¯ 32x32ï¼Œä½† x ç»´åº¦åªéœ€è¦åŸæ¥çš„ 1/4
    dim3 threadsPerBlock(32 / 4, 32); 
    dim3 numBlocks((N / 4 + threadsPerBlock.x - 1) / threadsPerBlock.x, 
                   (N + threadsPerBlock.y - 1) / threadsPerBlock.y);
    
    sgemm_vectorized_kernel<<<numBlocks, threadsPerBlock>>>(N, d_A, d_B, d_C);
}


// =========================================================
// Kernel 5: 2D Register Tiling (ç»ˆæä¼˜åŒ–)
// =========================================================

// è®¾å®šå—å¤§å°å‚æ•°
// BM, BN: ä¸€ä¸ª Block è®¡ç®— C çš„ 128x128 åŒºåŸŸ
// BK: K ç»´åº¦æ¯æ¬¡åˆ‡åˆ† 8
// TM, TN: æ¯ä¸ªçº¿ç¨‹è®¡ç®— C çš„ 8x8 åŒºåŸŸ
const int BM = 128;
const int BN = 128;
const int BK = 8;
const int TM = 8;
const int TN = 8;

__global__ void sgemm_2d_register_tiling_kernel(int N, float* A, float* B, float* C) {
    // 1. çº¿ç¨‹ä¸å—åæ ‡
    const int bx = blockIdx.x;
    const int by = blockIdx.y;
    const int tx = threadIdx.x;
    const int ty = threadIdx.y;
    
    // çº¿ç¨‹åœ¨ Block å†…çš„çº¿æ€§ ID (0 ~ 255)
    const int tid = ty * blockDim.x + tx;

    // 2. å£°æ˜ Shared Memory
    // As: [2][BK][BM] -> ä½¿ç”¨åŒç¼“å†²æ€è·¯é˜²æ­¢ Bank Conflict (è¿™é‡Œç®€åŒ–ä¸ºå•ç¼“å†²ä½†è½¬ç½®å­˜å‚¨)
    // ä¸ºäº†é¿å… Bank Conflictï¼Œæˆ‘ä»¬é€šå¸¸ä¼šæŠŠ shared memory è®¾å¤§ä¸€ç‚¹æˆ–è€…è½¬ç½®
    // è¿™é‡Œé‡‡ç”¨ç®€å•æ–¹æ¡ˆï¼šAs[BK][BM], Bs[BK][BN]
    // å®é™…ä¸Šå¯¹äº BM=128ï¼Œéœ€è¦å¾ˆå¤§çš„ Shared Memã€‚
    // æ³¨æ„ï¼š4060 çš„ Shared Memory è¶³å¤Ÿå¤§ (48KB/Block ä»¥ä¸Š)ã€‚
    __shared__ float As[BK][BM]; 
    __shared__ float Bs[BK][BN];

    // 3. å£°æ˜å¯„å­˜å™¨ (Register File)
    // æ¯ä¸ªçº¿ç¨‹è´Ÿè´£ 8x8 çš„ç´¯åŠ ç»“æœï¼Œè¿™ 64 ä¸ª float å¿…é¡»å®Œå…¨é©»ç•™åœ¨å¯„å­˜å™¨ä¸­
    float threadResults[TM][TN] = {0.0f};

    // 4. å¯„å­˜å™¨ç¼“å­˜ï¼Œç”¨äºä» Shared Mem è¯»å–æ•°æ®
    float regM[TM] = {0.0f};
    float regN[TN] = {0.0f};

    // 5. è®¡ç®—åŠ è½½ Global Memory çš„ç´¢å¼•
    // æˆ‘ä»¬éœ€è¦ç”± 256 ä¸ªçº¿ç¨‹ (16x16) åä½œæ¬è¿ A (128x8) å’Œ B (8x128)
    
    // A_row, A_col: å½“å‰çº¿ç¨‹è´Ÿè´£æ¬è¿ A çš„å“ªä¸ªç‚¹
    // A æ˜¯ 128è¡Œ x 8åˆ—ã€‚æ€»å…± 1024 ä¸ªå…ƒç´ ã€‚
    // çº¿ç¨‹æ•° 256ã€‚æ¯ä¸ªçº¿ç¨‹æ¬è¿ 4 ä¸ª floatã€‚
    // ä½¿ç”¨ float4 æ¬è¿ï¼
    const int load_a_row = tid / 2; // 0~127
    const int load_a_col = (tid % 2) * 4; // 0, 4

    // B_row, B_col: å½“å‰çº¿ç¨‹è´Ÿè´£æ¬è¿ B çš„å“ªä¸ªç‚¹
    // B æ˜¯ 8è¡Œ x 128åˆ—ã€‚æ€»å…± 1024 ä¸ªå…ƒç´ ã€‚
    // æ¯ä¸ªçº¿ç¨‹æ¬è¿ 4 ä¸ªã€‚
    const int load_b_row = tid / 32; // 0~7
    const int load_b_col = (tid % 32) * 4; // 0, 4, ..., 124

    // å¤§å¾ªç¯ï¼šéå† K ç»´åº¦
    for (int ph = 0; ph < N; ph += BK) {
        // --- 1. åä½œåŠ è½½ Global -> Shared ---
        
        // åŠ è½½ A (è½¬ç½®å­˜å…¥ Shared Mem ä»¥ä¼˜åŒ–è¯»å–) -> As[col][row]
        // ä½¿ç”¨ float4 å‘é‡åŒ–åŠ è½½
        float4 vecA = reinterpret_cast<float4*>(&A[(by * BM + load_a_row) * N + (ph + load_a_col)])[0];
        As[load_a_col][load_a_row] = vecA.x;
        As[load_a_col+1][load_a_row] = vecA.y;
        As[load_a_col+2][load_a_row] = vecA.z;
        As[load_a_col+3][load_a_row] = vecA.w;

        // åŠ è½½ B -> Bs[row][col]
        float4 vecB = reinterpret_cast<float4*>(&B[(ph + load_b_row) * N + (bx * BN + load_b_col)])[0];
        Bs[load_b_row][load_b_col] = vecB.x;
        Bs[load_b_row][load_b_col+1] = vecB.y;
        Bs[load_b_row][load_b_col+2] = vecB.z;
        Bs[load_b_row][load_b_col+3] = vecB.w;

        __syncthreads();

        // --- 2. æ ¸å¿ƒè®¡ç®— (å¯„å­˜å™¨çº§ GEMM) ---
        // å¤–å±‚å¾ªç¯ï¼šåœ¨ Shared Memory çš„ BK ç»´åº¦ä¸Šè¿­ä»£ (0~7)
        for (int k = 0; k < BK; ++k) {
            // å°† Shared Memory çš„æ•°æ®é¢„åŠ è½½åˆ°å¯„å­˜å™¨
            // è¿™ä¸€æ­¥æå¤§å‡å°‘äº† Shared Memory çš„è®¿é—®å‹åŠ›
            for (int m = 0; m < TM; ++m) {
                regM[m] = As[k][ty * TM + m];
            }
            for (int n = 0; n < TN; ++n) {
                regN[n] = Bs[k][tx * TN + n];
            }

            // å¤–ç§¯è®¡ç®— (Outer Product)
            // 8x8 = 64 æ¬¡ä¹˜åŠ ï¼Œçº¯å¯„å­˜å™¨æ“ä½œï¼Œæå¿«ï¼
            for (int m = 0; m < TM; ++m) {
                for (int n = 0; n < TN; ++n) {
                    threadResults[m][n] += regM[m] * regN[n];
                }
            }
        }

        __syncthreads();
    }

    // --- 3. å†™å›ç»“æœ ---
    // æ¯ä¸ªçº¿ç¨‹è´Ÿè´£å†™å› C çš„ 8x8 åŒºåŸŸ
    // è¿™ä¸€æ­¥ä¸éœ€è¦æè‡´ä¼˜åŒ–ï¼Œå› ä¸ºå®ƒåœ¨ Kernel ç»“æŸæ—¶åªæ‰§è¡Œä¸€æ¬¡
    for (int m = 0; m < TM; ++m) {
        for (int n = 0; n < TN; ++n) {
             int c_row = by * BM + ty * TM + m;
             int c_col = bx * BN + tx * TN + n;
             if (c_row < N && c_col < N) {
                 C[c_row * N + c_col] = threadResults[m][n];
             }
        }
    }
}

// Host è°ƒç”¨
void sgemm_gpu_2d_tiled_device(int N, float* d_A, float* d_B, float* d_C) {
    // è¿™é‡Œçš„ BlockSize æ˜¯çº¿ç¨‹æ•°çš„ç»´åº¦
    // æˆ‘ä»¬ç”¨ 16x16 = 256 ä¸ªçº¿ç¨‹
    // æ¯ä¸ªçº¿ç¨‹ç®— 8x8ï¼Œæ‰€ä»¥ä¸€ä¸ª Block ç®— (16*8) x (16*8) = 128x128
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / 128, N / 128); // å‡è®¾ N æ˜¯ 128 çš„å€æ•°

    sgemm_2d_register_tiling_kernel<<<numBlocks, threadsPerBlock>>>(N, d_A, d_B, d_C);
}