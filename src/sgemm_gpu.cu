#include <cublas_v2.h> // æ”¾åœ¨æœ€ä¸Šé¢
#include <cstdio>
#include <cuda_runtime.h>
#include "sgemm.h"

// å®ï¼šæ£€æŸ¥ CUDA é”™è¯¯ (éå¸¸é‡è¦ï¼ŒCUDA æŠ¥é”™ä¸åƒ C++ é‚£ä¹ˆç›´æ¥)
#define CHECK_CUDA(func) \
{ \
    cudaError_t status = (func); \
    if (status != cudaSuccess) { \
        printf("CUDA Error at line %d: %s\n", __LINE__, cudaGetErrorString(status)); \
        exit(EXIT_FAILURE); \
    } \
}

// å®šä¹‰ Block å¤§å°ï¼Œå¿…é¡»å’Œ Host ç«¯è®¾ç½®çš„ä¸€æ ·
#define BLOCK_SIZE 16

// -----------------------------------------------------------
// 1. Kernel å‡½æ•°: åœ¨ GPU ä¸Šæ‰§è¡Œçš„ä»£ç 
// __global__ è¡¨ç¤º: Host(CPU) è°ƒç”¨ï¼ŒDevice(GPU) æ‰§è¡Œ
// -----------------------------------------------------------
__global__ void sgemm_naive_kernel(int N, float* A, float* B, float* C) {
    // æ¯ä¸€ä¸ªçº¿ç¨‹è®¡ç®— C çš„ä¸€ä¸ªå…ƒç´  C[row][col]
    
    // è®¡ç®—å½“å‰çº¿ç¨‹çš„åæ ‡
    // blockIdx: å½“å‰å±äºå“ªä¸ªæ–¹å—
    // blockDim: ä¸€ä¸ªæ–¹å—æœ‰å¤šå¤§
    // threadIdx: åœ¨æ–¹å—é‡Œçš„ç¼–å·
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < N && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < N; ++k) {
            sum += A[row * N + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}

// -----------------------------------------------------------
// 2. Host å‡½æ•°: è´Ÿè´£åˆ†é…æ˜¾å­˜ã€æ‹·è´æ•°æ®ã€å¯åŠ¨ Kernel
// -----------------------------------------------------------
void sgemm_gpu_naive(int N, float* A, float* B, float* C) {
    size_t bytes = N * N * sizeof(float);

    // 1. åœ¨ GPU (Device) ä¸Šåˆ†é…å†…å­˜
    float *d_A, *d_B, *d_C;
    CHECK_CUDA(cudaMalloc(&d_A, bytes));
    CHECK_CUDA(cudaMalloc(&d_B, bytes));
    CHECK_CUDA(cudaMalloc(&d_C, bytes));

    // 2. å°†æ•°æ®ä» CPU (Host) æ‹·è´åˆ° GPU (Device)
    // æ³¨æ„: cudaMemcpy æ˜¯åŒæ­¥æ“ä½œï¼Œæ¯”è¾ƒæ…¢ï¼Œä¼šè¢«è®¡å…¥æ—¶é—´
    CHECK_CUDA(cudaMemcpy(d_A, A, bytes, cudaMemcpyHostToDevice));
    CHECK_CUDA(cudaMemcpy(d_B, B, bytes, cudaMemcpyHostToDevice));
    
    // 3. é…ç½®å¯åŠ¨å‚æ•°
    // æ¯ä¸ª Block å¤§å°ä¸º 32x32 (1024ä¸ªçº¿ç¨‹ï¼Œè¿™æ˜¯ä¸Šé™)
    dim3 threadsPerBlock(32, 32);
    // Grid å¤§å°æ ¹æ® N åŠ¨æ€è®¡ç®—
    dim3 numBlocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x, 
                   (N + threadsPerBlock.y - 1) / threadsPerBlock.y);

    // 4. å¯åŠ¨ Kernel
    sgemm_naive_kernel<<<numBlocks, threadsPerBlock>>>(N, d_A, d_B, d_C);

    // æ£€æŸ¥ Kernel æ˜¯å¦å¯åŠ¨å¤±è´¥
    CHECK_CUDA(cudaGetLastError());
    
    // ç­‰å¾… GPU æ‰§è¡Œå®Œæ¯• (å› ä¸º Kernel æ˜¯å¼‚æ­¥çš„)
    CHECK_CUDA(cudaDeviceSynchronize());

    // 5. å°†ç»“æœä» GPU æ‹·å› CPU
    CHECK_CUDA(cudaMemcpy(C, d_C, bytes, cudaMemcpyDeviceToHost));

    // 6. é‡Šæ”¾æ˜¾å­˜
    CHECK_CUDA(cudaFree(d_A));
    CHECK_CUDA(cudaFree(d_B));
    CHECK_CUDA(cudaFree(d_C));
}


// -----------------------------------------------------------
// 2. Shared Memory Tiling Kernel
// -----------------------------------------------------------
__global__ void sgemm_shared_mem_kernel(int N, float* A, float* B, float* C) {
    // blockIdx: å½“å‰ Block çš„åæ ‡
    // threadIdx: å½“å‰çº¿ç¨‹åœ¨ Block å†…çš„åæ ‡
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    // è®¡ç®—å½“å‰çº¿ç¨‹è´Ÿè´£è®¡ç®— C ä¸­çš„å“ªä¸ªå…ƒç´ åæ ‡
    int row = by * BLOCK_SIZE + ty;
    int col = bx * BLOCK_SIZE + tx;

    // å£°æ˜ Shared Memory (è¿™æ˜¯ Block å†…æ‰€æœ‰çº¿ç¨‹å…±äº«çš„)
    // å¤§å°æ˜¯ 32x32 çš„ float çŸ©é˜µ
    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];

    float Cvalue = 0.0f;

    // æ ¸å¿ƒå¾ªç¯ï¼šå°†å¤§çŸ©é˜µæ‹†åˆ†æˆä¸€ä¸ªä¸ª BLOCK_SIZE å®½åº¦çš„â€œæ¡â€ (Tile) æ¥éå†
    // ph (Phase) ä»£è¡¨å½“å‰å¤„ç†ç¬¬å‡ ä¸ª Tile
    for (int ph = 0; ph < N / BLOCK_SIZE; ++ph) {
        
        // --- 1. åä½œåŠ è½½æ•°æ®åˆ° Shared Memory ---
        
        // æ¯ä¸ªçº¿ç¨‹è´Ÿè´£æ¬è¿ A çŸ©é˜µçš„ä¸€ä¸ªç‚¹ï¼šA[row][ph * BLOCK_SIZE + tx]
        As[ty][tx] = A[row * N + ph * BLOCK_SIZE + tx];
        
        // æ¯ä¸ªçº¿ç¨‹è´Ÿè´£æ¬è¿ B çŸ©é˜µçš„ä¸€ä¸ªç‚¹ï¼šB[ph * BLOCK_SIZE + ty][col]
        Bs[ty][tx] = B[(ph * BLOCK_SIZE + ty) * N + col];

        // ğŸš§ çº¿ç¨‹åŒæ­¥æ …æ  (å¿…è€ƒç‚¹!) ğŸš§
        // å¿…é¡»ç­‰å¾… Block å†…æ‰€æœ‰çº¿ç¨‹éƒ½æŠŠæ•°æ®æ¬å®Œäº†ï¼Œæ‰èƒ½å¼€å§‹è®¡ç®—
        __syncthreads();

        // --- 2. åœ¨ Shared Memory ä¸Šè¿›è¡Œè®¡ç®— ---
        
        for (int k = 0; k < BLOCK_SIZE; ++k) {
            // ç°åœ¨æ˜¯ä»é«˜é€Ÿçš„ Shared Memory (As, Bs) å–æ•°ï¼Œè€Œä¸æ˜¯æ…¢é€Ÿçš„ A, B
            Cvalue += As[ty][k] * Bs[k][tx];
        }

        // ğŸš§ å†æ¬¡åŒæ­¥ ğŸš§
        // å¿…é¡»ç­‰å¾…æ‰€æœ‰çº¿ç¨‹éƒ½ç®—å®Œäº†å½“å‰è¿™ä¸ª Tileï¼Œæ‰èƒ½è¿›å…¥ä¸‹ä¸€è½®å¾ªç¯å»è¦†ç›– As, Bs
        __syncthreads();
    }

    // å†™å›ç»“æœ
    if (row < N && col < N) {
        C[row * N + col] = Cvalue;
    }
}

// å¯¹åº”çš„ Host è°ƒç”¨å‡½æ•°
void sgemm_gpu_shared(int N, float* A, float* B, float* C) {
    size_t bytes = N * N * sizeof(float);
    float *d_A, *d_B, *d_C;

    CHECK_CUDA(cudaMalloc(&d_A, bytes));
    CHECK_CUDA(cudaMalloc(&d_B, bytes));
    CHECK_CUDA(cudaMalloc(&d_C, bytes));

    CHECK_CUDA(cudaMemcpy(d_A, A, bytes, cudaMemcpyHostToDevice));
    CHECK_CUDA(cudaMemcpy(d_B, B, bytes, cudaMemcpyHostToDevice));

    dim3 threadsPerBlock(BLOCK_SIZE, BLOCK_SIZE);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);

    // è°ƒç”¨æ–°çš„ Kernel
    sgemm_shared_mem_kernel<<<numBlocks, threadsPerBlock>>>(N, d_A, d_B, d_C);

    CHECK_CUDA(cudaGetLastError());
    CHECK_CUDA(cudaDeviceSynchronize());
    CHECK_CUDA(cudaMemcpy(C, d_C, bytes, cudaMemcpyDeviceToHost));

    CHECK_CUDA(cudaFree(d_A));
    CHECK_CUDA(cudaFree(d_B));
    CHECK_CUDA(cudaFree(d_C));
}


// 3. CuBLAS ç‰ˆæœ¬ (å®˜æ–¹é—­æºåº“)
void sgemm_gpu_cublas(int N, float* A, float* B, float* C) {
    cublasHandle_t handle;
    cublasCreate(&handle);

    float alpha = 1.0f;
    float beta = 1.0f; // æ³¨æ„è¿™é‡Œ beta=1ï¼Œæ„å‘³ç€ C += A*Bï¼Œç¬¦åˆæˆ‘ä»¬çš„ benchmark é€»è¾‘
    // å¦‚æœæƒ³è¦çº¯å‡€çš„ C = A*Bï¼Œbeta åº”è¯¥è®¾ä¸º 0ï¼Œä¸”å¤–éƒ¨ benchmark éœ€è¦æ¸…é›¶ C

    // å…³é”®ç‚¹ï¼šCuBLAS é»˜è®¤æ˜¯åˆ—ä¸»åº (Column Major)ï¼Œè€Œæˆ‘ä»¬æ˜¯è¡Œä¸»åº (Row Major)ã€‚
    // C = A * B (Row Major) ç­‰ä»·äº C^T = B^T * A^T (Column Major)
    // æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬éœ€è¦â€œéª—â€ä¸€ä¸‹ CuBLASï¼š
    // ä¼ è¿›å» B å½“ä½œ Aï¼Œä¼ è¿›å» A å½“ä½œ Bï¼Œæœ€åç®—å‡ºæ¥çš„ç»“æœç›´æ¥å°±æ˜¯è¡Œä¸»åºçš„ Cã€‚
    
    // è§£é‡Šå‚æ•°ï¼š
    // Handle, OP_N (ä¸è½¬ç½®), OP_N (ä¸è½¬ç½®), 
    // M=N, N=N, K=N (çŸ©é˜µå¤§å°),
    // alpha, 
    // B (ä½œä¸ºç¬¬ä¸€ä¸ªçŸ©é˜µ), ldb=N, 
    // A (ä½œä¸ºç¬¬äºŒä¸ªçŸ©é˜µ), lda=N, 
    // beta, 
    // C (ç»“æœ), ldc=N
    
    // æ˜¾å­˜æŒ‡é’ˆ
    size_t bytes = N * N * sizeof(float);
    float *d_A, *d_B, *d_C;
    cudaMalloc(&d_A, bytes);
    cudaMalloc(&d_B, bytes);
    cudaMalloc(&d_C, bytes);
    
    cudaMemcpy(d_A, A, bytes, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, B, bytes, cudaMemcpyHostToDevice);
    // C è¿™é‡Œå¦‚æœæ˜¯ç´¯åŠ ï¼Œéœ€è¦æŠŠ Host çš„ C æ‹·è¿›å»ï¼›å¦‚æœæ˜¯è¦†ç›–ï¼Œåˆ™ä¸éœ€è¦ã€‚
    // ä¸ºäº†å…¬å¹³ï¼Œå‡è®¾æ˜¯è¦†ç›– (beta=0) æˆ–ç´¯åŠ ã€‚è¿™é‡Œç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾æ˜¯çº¯è®¡ç®—ã€‚
    // ä¿®æ­£ï¼šbenchmark loop é‡Œæˆ‘ä»¬é€šå¸¸æŠŠ C è®¾ä¸º 0ï¼Œæ‰€ä»¥è¿™é‡Œ beta=0 æ¯”è¾ƒåˆé€‚ã€‚
    float beta_overwrite = 0.0f;

    cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, 
                N, N, N, 
                &alpha, 
                d_B, N, // B æ¢åˆ°å‰é¢
                d_A, N, // A æ¢åˆ°åé¢
                &beta_overwrite, 
                d_C, N);

    cudaDeviceSynchronize(); // ç­‰å¾…è®¡ç®—å®Œæˆ

    cudaMemcpy(C, d_C, bytes, cudaMemcpyDeviceToHost);

    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);
    cublasDestroy(handle);
}


// =========================================================
// æ–°å¢ï¼šçº¯è®¡ç®—æ¥å£ (Device Pointers Only)
// è¿™äº›å‡½æ•°å‡è®¾ d_A, d_B, d_C å·²ç»åœ¨æ˜¾å­˜é‡Œäº†ï¼Œåªè´Ÿè´£è®¡ç®—
// =========================================================

// 1. Naive Device æ¥å£
void sgemm_gpu_naive_device(int N, float* d_A, float* d_B, float* d_C) {
    dim3 threadsPerBlock(32, 32);
    dim3 numBlocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x, 
                   (N + threadsPerBlock.y - 1) / threadsPerBlock.y);
    sgemm_naive_kernel<<<numBlocks, threadsPerBlock>>>(N, d_A, d_B, d_C);
}

// 2. Shared Memory Device æ¥å£ (Block Size = 16)
void sgemm_gpu_shared_device(int N, float* d_A, float* d_B, float* d_C) {
    // å¼ºåˆ¶ä½¿ç”¨ Block Size 16 (é…åˆä¹‹å‰çš„å®å®šä¹‰)
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);
    sgemm_shared_mem_kernel<<<numBlocks, threadsPerBlock>>>(N, d_A, d_B, d_C);
}

// 3. CuBLAS Device æ¥å£
void sgemm_gpu_cublas_device(int N, float* d_A, float* d_B, float* d_C) {
    // ä¸ºäº†æ€§èƒ½ï¼Œhandle åº”è¯¥åœ¨å¤–éƒ¨åˆ›å»ºï¼Œä½†è¿™é‡Œä¸ºäº†æ¥å£ç®€å•å…ˆæ”¾åœ¨è¿™
    // æ³¨æ„ï¼šé¢‘ç¹åˆ›å»º handle ä¹Ÿæœ‰å¼€é”€ï¼Œä½†åœ¨å¤§çŸ©é˜µä¸‹å¯ä»¥å¿½ç•¥
    cublasHandle_t handle;
    cublasCreate(&handle);
    float alpha = 1.0f;
    float beta = 0.0f; 

    // å†æ¬¡æé†’ï¼šCuBLAS æ˜¯åˆ—ä¸»åºï¼Œæˆ‘ä»¬äº¤æ¢ A/B æ¥æ¬ºéª—å®ƒ
    cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, 
                N, N, N, 
                &alpha, 
                d_B, N, 
                d_A, N, 
                &beta, 
                d_C, N);
    
    // è¿™é‡Œä¸éœ€è¦ DeviceSynchronizeï¼Œå› ä¸º benchmark ä¸»ç¨‹åºä¼šåš
    cublasDestroy(handle);
}